{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class ChaniaDataset(Dataset):\n",
    "    def __init__(self, data_mat, num_features, transform=None, normalize=True):\n",
    "        self.num_features = num_features\n",
    "        self.data_mat = sio.loadmat(data_mat)\n",
    "        self.augmented_data = self.data_mat[\"data\"].astype(\"float\")\n",
    "        self.userlabels = self.data_mat[\"label\"].astype(\"float\")\n",
    "        self.transform = transform\n",
    "\n",
    "        if normalize:\n",
    "            self.augmented_data=(self.augmented_data-self.augmented_data.mean(axis=0))/self.augmented_data.std(axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.augmented_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        data = self.augmented_data[idx].reshape(-1,self.num_features)\n",
    "        user = self.userlabels[idx].reshape(-1,1)\n",
    "        sample = {'x':data, 'u':user}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        data, user = sample['x'], sample['u']\n",
    "        return {'x':torch.from_numpy(data), 'u':torch.from_numpy(user)}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS\n",
    "\n",
    "def poly(degree, long, lat):\n",
    "    return torch.cat([long**i*lat**(degree-i) for degree in range(degree+1) for i in range(degree,-1,-1)],1)\n",
    "\n",
    "def signal_map_params(x,degree):\n",
    "    polynomial = poly(degree, x[:,:,13], x[:,:,12])\n",
    "    beta = torch.mm(torch.inverse(torch.mm(torch.transpose(polynomial,0,1), polynomial)),\n",
    "                  torch.mm(torch.transpose(polynomial,0,1), x[:,:,6]))\n",
    "    return beta\n",
    "\n",
    "def density_count(x, num_grids):\n",
    "    count = torch.zeros(num_grids,num_grids)\n",
    "    x1min=torch.min(x[:,:,13])\n",
    "    x2min=torch.min(x[:,:,12])\n",
    "    size1 = torch.max(x[:,:,13])-x1min\n",
    "    size2 = torch.max(x[:,:,12])-x2min\n",
    "    a_all = []\n",
    "    c_all = []\n",
    "    for i in range(num_grids):\n",
    "        for j in range(num_grids):\n",
    "            a = x1min+(size1/num_grids*i)\n",
    "            a_all.append(a)\n",
    "            b = x1min+(size1/num_grids*(i+1))\n",
    "            a_all.append(b)\n",
    "            c = x2min+(size2/num_grids*j)\n",
    "            c_all.append(c)\n",
    "            d = x2min+(size2/num_grids*(j+1))\n",
    "            c_all.append(d)\n",
    "            if i == 0 and j != num_grids-1:\n",
    "                count[i][j] += x[(x[:,:,13] >= a ) & (x[:,:,13] < b) & (x[:,:,12] >= c) & (x[:,:,12] <= d)].size(0)\n",
    "            elif i == 0 and j == num_grids-1:\n",
    "                count[i][j] += x[(x[:,:,13] >= a) & (x[:,:,13] <= b) & (x[:,:,12] >= c) & (x[:,:,12] <= d)].size(0)\n",
    "            elif i != 0 and j == num_grids-1:\n",
    "                count[i][j] += x[(x[:,:,13] >= a) & (x[:,:,13] <= b) & (x[:,:,12] >= c) & (x[:,:,12] < d)].size(0)\n",
    "            else:\n",
    "                count[i][j] += x[(x[:,:,13] >= a ) & (x[:,:,13] < b) & (x[:,:,12] >= c) & (x[:,:,12] < d)].size(0)\n",
    "    return count, torch.unique(torch.Tensor(a_all)), torch.unique(torch.Tensor(c_all))\n",
    "\n",
    "def density_loss(x,y, batch_size):\n",
    "    a = (100*torch.clamp(x[:,:,12],0,0.01).sum() - 100*torch.clamp(y[:,:,12],0,0.01).sum())\n",
    "    b = (-100*torch.clamp(x[:,:,12],-0.01,0).sum() - -100*torch.clamp(y[:,:,12],-0.01,0).sum())\n",
    "    c = (100*torch.clamp(x[:,:,13],0,0.01).sum() - 100*torch.clamp(y[:,:,13],0,0.01).sum())\n",
    "    d = (-100*torch.clamp(x[:,:,13],-0.01,0).sum() - -100*torch.clamp(y[:,:,13],-0.01,0).sum())\n",
    "    return (a+b+c+d)/batch_size\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.3)\n",
    "        # m.bias.data.fill_(2)\n",
    "\n",
    "def analytical_gaussian_sigma(eta, epsilon, delta):\n",
    "    def phi(x):\n",
    "        return 1/2*(1+math.erf(x/math.sqrt(2)))\n",
    "    delta_0 = phi(0)-math.e**epsilon*phi(-math.sqrt(2*epsilon))\n",
    "    def Bplus(v):\n",
    "        return phi(math.sqrt(epsilon*v))-math.e**epsilon*phi(-math.sqrt(epsilon*(v+2)))\n",
    "    def Bminus(v):\n",
    "        return phi(-math.sqrt(epsilon*v))-math.e**epsilon*phi(-math.sqrt(epsilon*(v+2)))\n",
    "    if delta >= delta_0:\n",
    "        vstar = 0\n",
    "        while Bplus(vstar) <= delta:\n",
    "            vstar += 1\n",
    "            if vstar == 1000000:\n",
    "                return 0\n",
    "        alpha = math.sqrt(1+vstar/2)-math.sqrt(vstar/2)\n",
    "    else:\n",
    "        ustar = 0\n",
    "        while Bminus(ustar) > delta:\n",
    "            ustar += 1\n",
    "        alpha = math.sqrt(1+ustar/2)+math.sqrt(ustar/2)\n",
    "    sigma = alpha*eta/math.sqrt(2*epsilon)\n",
    "    return sigma\n",
    "\n",
    "## LOSS FUNCTIONS\n",
    "\n",
    "def make_privatizer_loss(map_params, num_grids, batch_size, utility_weights, rho):\n",
    "    def utility_loss(x,y):\n",
    "        bx = signal_map_params(x,map_params)\n",
    "        by = signal_map_params(y,map_params)\n",
    "        l1 = sum(abs(bx-by))\n",
    "        # l2 = (x-y).pow(2).mean() # mean squared error\n",
    "        l2 = (x.view(batch_size,-1)-y.view(batch_size,-1)).pow(2).sum(1).pow(0.5).mean() # average distance\n",
    "        # l3 = (y[:,:,12:14]-x[:,:,12:14]).pow(2).mean() # mean squared error\n",
    "        l3 = (y[:,:,12:14]-x[:,:,12:14]).pow(2).sum(1).pow(0.5).mean() # average distance\n",
    "        cx,_,_ = density_count(x,num_grids)\n",
    "        cy,_,_ = density_count(y,num_grids)\n",
    "        l4 = ((abs(cx-cy)).sum()+(cx.sum()-cy.sum()))/(2*cx.sum())\n",
    "        l6 = density_loss(x,y, batch_size)\n",
    "        return l1, l2, l3, l4, l6\n",
    "    def privatizer_loss(x,y,u,uhat,lochat):\n",
    "        l1, l2, l3, l4, l6 = utility_loss(x,y)\n",
    "        w1,w2,w3,w4 = utility_weights\n",
    "        total_loss = rho*(w1*l1+w2*l2+w3*l3+w4*l6)-(1-rho)*adversary_loss(u,uhat)\n",
    "        return total_loss\n",
    "    return utility_loss, privatizer_loss\n",
    "\n",
    "def make_adversary_loss():\n",
    "    def adversary_loss(u,uhat):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        print(uhat.size(),u.size())\n",
    "        return criterion(uhat,u)\n",
    "    return adversary_loss\n",
    "\n",
    "## ADVERSARY\n",
    "\n",
    "def make_adversary(num_features, num_units, num_users):\n",
    "    adversary = torch.nn.Sequential(\n",
    "        torch.nn.Linear(num_features, num_units),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(num_units, num_units),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(num_units, num_users)\n",
    "    )\n",
    "    # adversary.apply(init_weights)\n",
    "    adversary.double()\n",
    "    adversary_optimizer = optim.Adam(adversary.parameters(),lr=0.001, betas=(0.9,0.999))\n",
    "    return adversary, adversary_optimizer\n",
    "\n",
    "## PRIVATIZER\n",
    "\n",
    "def make_gap_privatizer(num_features, num_units):\n",
    "    gap_privatizer = torch.nn.Sequential(\n",
    "        torch.nn.Linear(num_features, num_units),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(num_units, num_units),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(num_units, num_features)\n",
    "    )\n",
    "    # gap_privatizer.apply(init_weights)\n",
    "    gap_privatizer.double()\n",
    "    gap_privatizer_optimizer = optim.Adam(gap_privatizer.parameters(),lr=0.001, betas=(0.9,0.999))\n",
    "    return gap_privatizer, gap_privatizer_optimizer\n",
    "\n",
    "def dp_privatizer(x, s, norm_clip):\n",
    "    normvec = torch.norm(x,p=2,dim=2)\n",
    "    scalevec = norm_clip/normvec\n",
    "    scalevec[scalevec>1] = 1\n",
    "    x = torch.transpose(torch.transpose(x,0,1)*scalevec,0,1).double()\n",
    "    noise = torch.normal(mean=torch.zeros_like(x),std=s).double()\n",
    "    y = x + noise\n",
    "    return y\n",
    "\n",
    "def noise_privatizer(x, sigma):\n",
    "    noise = torch.normal(mean=torch.zeros_like(x),std=sigma).double()\n",
    "    y = x + noise\n",
    "    return y\n",
    "\n",
    "def make_codebook(codebook_size, batch_size, num_features):\n",
    "    prob_distr = gaussian_kde(chania_dataset.augmented_data.values.T)\n",
    "    codebook = {}\n",
    "    for i in range(codebook_size):\n",
    "        y = prob_distr.resample(batch_size).T\n",
    "        y = torch.DoubleTensor(y.reshape(batch_size,1,num_features))\n",
    "        codebook[y] = None\n",
    "    return codebook\n",
    "\n",
    "def MI_privatizer(x, codebook, codebook_multiplier, utility_loss):\n",
    "    for y in codebook.keys():\n",
    "        loss_utility = utility_loss(x,y)[:-1]\n",
    "        codebook[y] = sum(loss_utility).item()\n",
    "    options = list(codebook.keys())\n",
    "    options.append(x)\n",
    "    weights = list(map(lambda x: math.e**(-x*codebook_multiplier), codebook.values()))\n",
    "    weights.append(1)\n",
    "    best = random.choices(options, weights)[0]\n",
    "    # if (best-y).sum().item() != 0:\n",
    "    #     print(\"chose a different batch\")\n",
    "    return best\n",
    "\n",
    "## TRAIN_SPLIT\n",
    "def train(num_epochs, train_loader, PRIVATIZER, gap_privatizer, gap_privatizer_optimizer, codebook, codebook_multiplier, utility_loss, privatizer_loss, sigma_dp, norm_clip, sigma_gaussian, adversary_optimizer, adversary, adversary_loss, batch_size, num_users):\n",
    "    convergence_threshold = 0.0001\n",
    "    previous_aloss, previous_ploss = -1, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch\",epoch)\n",
    "        # iterate through the training dataset\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # unpack batch\n",
    "            x, u = batch['x'], batch['u'].squeeze()\n",
    "            if x.shape[0] != batch_size:\n",
    "                break\n",
    "            # generate privatized batch\n",
    "            if PRIVATIZER == \"gap_privatizer\":\n",
    "                # reset privatizer gradients\n",
    "                gap_privatizer_optimizer.zero_grad()\n",
    "                y = gap_privatizer(x)\n",
    "            elif PRIVATIZER == \"MI_privatizer\":\n",
    "                y = MI_privatizer(x, codebook, codebook_multiplier, utility_loss)\n",
    "            elif PRIVATIZER == \"dp_privatizer\":\n",
    "                y = dp_privatizer(x,sigma_dp, norm_clip)\n",
    "            elif PRIVATIZER == \"noise_privatizer\":\n",
    "                y = noise_privatizer(x, sigma_gaussian)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            # reset adversary gradients\n",
    "            adversary_optimizer.zero_grad()\n",
    "            uhat = adversary(y).squeeze()\n",
    "#             uhat, lochat = estimate[:,:num_users], estimate[:,num_users:]\n",
    "\n",
    "            # train adversary\n",
    "            if i%10 < 5:\n",
    "                aloss = adversary_loss(u,uhat)\n",
    "                aloss.backward(retain_graph=True) # to do: is this necessary?\n",
    "                torch.nn.utils.clip_grad_norm_(adversary.parameters(), 1000)\n",
    "                adversary_optimizer.step()\n",
    "                if abs(aloss.item()-previous_aloss) < convergence_threshold:\n",
    "                    print(\"adversary converged\", i)\n",
    "                    if PRIVATIZER != \"gap_privatizer\":\n",
    "                        return\n",
    "                previous_aloss = aloss.item()\n",
    "\n",
    "            # train privatizer\n",
    "            if PRIVATIZER == \"gap_privatizer\":\n",
    "                if i%10 >= 5:\n",
    "                    ploss = privatizer_loss(x,y,u,uhat)\n",
    "                    ploss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(gap_privatizer.parameters(), 1000)\n",
    "                    gap_privatizer_optimizer.step()\n",
    "                    if abs(ploss.item()-previous_ploss) < convergence_threshold:\n",
    "                        print(\"privatizer converged\",i)\n",
    "                        if abs(aloss.item()-previous_aloss) < convergence_threshold:\n",
    "                            print(\"done\", i)\n",
    "                            return\n",
    "                    previous_ploss = ploss.item()\n",
    "\n",
    "            # print progress\n",
    "            if i % 50 == 49:\n",
    "                # evaluate utility loss\n",
    "                aloss = adversary_loss(u,uhat)\n",
    "                loss_utility = utility_loss(x,y)[:-1]\n",
    "                print(i+1,\"aloss:\",aloss.item(),\"uloss:\",sum(loss_utility).item())\n",
    "                if PRIVATIZER == \"gap_privatizer\":\n",
    "                    ploss = privatizer_loss(x,y,u,uhat,lochat)\n",
    "                    print(\"ploss:\",ploss.item())\n",
    "\n",
    "    print(\"done\", i)\n",
    "\n",
    "def test(test_loader, test_epochs, PRIVATIZER, gap_privatizer_optimizer, gap_privatizer, codebook, codebook_multiplier, utility_loss, privatizer_loss, sigma_dp, norm_clip, sigma_gaussian, adversary, map_params, num_grids, batch_size, num_users):\n",
    "    correct = 0\n",
    "    loc_error = 0\n",
    "    l1,l2,l3,l4,l5,l6 = 0,0,0,0,0,0\n",
    "\n",
    "    # iterate through test data\n",
    "    for epoch in range(test_epochs):\n",
    "        for i,batch in enumerate(test_loader):\n",
    "\n",
    "            # unpack batch\n",
    "            x, u = batch['x'], batch['u'].squeeze()\n",
    "            if x.shape[0] != batch_size:\n",
    "                break\n",
    "            # generate privatized batch\n",
    "            if PRIVATIZER == \"gap_privatizer\":\n",
    "                # reset privatizer gradients\n",
    "                gap_privatizer_optimizer.zero_grad()\n",
    "                y = gap_privatizer(x)\n",
    "            elif PRIVATIZER == \"MI_privatizer\":\n",
    "                y = MI_privatizer(x, codebook, codebook_multiplier, utility_loss)\n",
    "            elif PRIVATIZER == \"dp_privatizer\":\n",
    "                y = dp_privatizer(x,sigma_dp, norm_clip)\n",
    "            elif PRIVATIZER == \"noise_privatizer\":\n",
    "                y = noise_privatizer(x, sigma_gaussian)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            # estimate userID and location\n",
    "            estimate = adversary(y).squeeze()\n",
    "            uhat, lochat = estimate[:,:num_users], estimate[:,num_users:]\n",
    "\n",
    "            # Privacy Metric\n",
    "            _, upred = torch.max(uhat.data,1)\n",
    "            correct+=(upred==u).sum().item()/batch_size\n",
    "            loc_error += (x[:,:,12:14].squeeze()-lochat).pow(2).sum(1).pow(0.5).mean().item()\n",
    "\n",
    "            # Utility Metrics\n",
    "            this_l1, this_l2, this_l3, this_l4, this_l6 = utility_loss(x,y)\n",
    "            l1 += this_l1\n",
    "            l2 += this_l2\n",
    "            l3 += this_l3\n",
    "            l4 += this_l4\n",
    "            l6 += this_l6\n",
    "\n",
    "    return 100*correct/(i+1)/test_epochs, loc_error/(i+1)/test_epochs, l1.item()/(i+1)/test_epochs, l2.item()/(i+1)/test_epochs, l3.item()/(i+1)/test_epochs, l4.item()/(i+1)/test_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'Data_L_5.mat'\n",
    "\n",
    "# BATCH_SIZE = 16 # todo\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_SPLIT = 0.7\n",
    "NUM_LEN = 5\n",
    "NUM_FEATURES = 25 * NUM_LEN\n",
    "NUM_UNITS = 32\n",
    "NUM_USERS = 6\n",
    "NUM_EPOCHS = 5\n",
    "TEST_EPOCHS = 3\n",
    "\n",
    "DELTA = 0.00001\n",
    "NORM_CLIP=7.154\n",
    "\n",
    "UTILITY_WEIGHTS = (1,1,1,1)\n",
    "PRIVACY_WEIGHTS =(1,1)\n",
    "MAP_PARAMS = 2\n",
    "NUM_GRIDS = 16\n",
    "\n",
    "CODEBOOK_SIZE = 5 # todo\n",
    "\n",
    "userID = {\n",
    "'a841f74e620f74ec443b7a25d7569545':0,\n",
    "'22223276ea84bbce3a62073c164391fd':1,\n",
    "'7cbc37da05801d46e7d80c3b99fd5adb':2,\n",
    "'6882f6cf8c72d6324ba7e6bb42c9c7c2':3,\n",
    "'1e33db5d2be36268b944359fbdbdad21':4,\n",
    "'892d2c3aae6e51f23bf8666c2314b52f':5,\n",
    "# '510635002cb29804d54bff664cab52be':6,\n",
    "# '7023889b4439d2c02977ba152d6f4c6e':7,\n",
    "# '8425a81da55ec16b7f9f80c139c235a2':8,\n",
    "}\n",
    "\n",
    "chania_dataset = ChaniaDataset(data_mat=FILENAME, num_features=NUM_FEATURES, transform=ToTensor(), normalize=True)\n",
    "train_size=int(TRAIN_SPLIT*len(chania_dataset))\n",
    "test_size = len(chania_dataset)-train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(chania_dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "torch.Size([16, 6]) torch.Size([16, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Double for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-43d7d9068787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0madversary_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_adversary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msigma_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalytical_gaussian_sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNORM_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRIVATIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_privatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_privatizer_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCODEBOOK_MULTIPLIER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivatizer_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIGMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversary_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversary_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_USERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistortion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRIVATIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_privatizer_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_privatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCODEBOOK_MULTIPLIER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivatizer_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIGMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAP_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_GRIDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_USERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-769a500ccb60>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, train_loader, PRIVATIZER, gap_privatizer, gap_privatizer_optimizer, codebook, codebook_multiplier, utility_loss, privatizer_loss, sigma_dp, norm_clip, sigma_gaussian, adversary_optimizer, adversary, adversary_loss, batch_size, num_users)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# train adversary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0maloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0maloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to do: is this necessary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-769a500ccb60>\u001b[0m in \u001b[0;36madversary_loss\u001b[0;34m(u, uhat)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madversary_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Double for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "## rho of 0 is private, 1 is useful\n",
    "PRIVATIZER = \"gap_privatizer\"\n",
    "EPSILON, SIGMA, CODEBOOK_MULTIPLIER = 0, 0, 0\n",
    "for RHO in [0,0.001,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "\n",
    "    adversary, adversary_optimizer = make_adversary(NUM_FEATURES, NUM_UNITS, NUM_USERS)\n",
    "    if PRIVATIZER == \"gap_privatizer\":\n",
    "        gap_privatizer, gap_privatizer_optimizer = make_gap_privatizer(NUM_FEATURES, NUM_UNITS)\n",
    "    else:\n",
    "        gap_privatizer, gap_privatizer_optimizer = None, None\n",
    "    if PRIVATIZER == \"MI_privatizer\":\n",
    "        codebook = make_codebook(CODEBOOK_SIZE, BATCH_SIZE, NUM_FEATURES)\n",
    "    else:\n",
    "        codebook = None\n",
    "    utility_loss, privatizer_loss = make_privatizer_loss(MAP_PARAMS, NUM_GRIDS, BATCH_SIZE, UTILITY_WEIGHTS, RHO)\n",
    "\n",
    "    adversary_loss = make_adversary_loss()\n",
    "    sigma_dp = analytical_gaussian_sigma(NORM_CLIP, EPSILON, DELTA)\n",
    "    train(NUM_EPOCHS, train_loader, PRIVATIZER, gap_privatizer, gap_privatizer_optimizer, codebook, CODEBOOK_MULTIPLIER, utility_loss, privatizer_loss, sigma_dp, NORM_CLIP, SIGMA, adversary_optimizer, adversary, adversary_loss, BATCH_SIZE, NUM_USERS)\n",
    "    acc, loc_error, map_error, distortion, dist_error, density_error = test(test_loader, TEST_EPOCHS, PRIVATIZER, gap_privatizer_optimizer, gap_privatizer, codebook, CODEBOOK_MULTIPLIER, utility_loss, privatizer_loss, sigma_dp, NORM_CLIP, SIGMA, adversary, MAP_PARAMS, NUM_GRIDS, BATCH_SIZE, NUM_USERS)\n",
    "\n",
    "    RESULT_FILENAME = PRIVATIZER+\".csv\"\n",
    "    with open(RESULT_FILENAME, \"a\") as fd:\n",
    "        fd.write(PRIVATIZER)\n",
    "        fd.write(\",\")\n",
    "        if PRIVATIZER == \"gap_privatizer\":\n",
    "            print(PRIVATIZER,\"RHO=\",RHO)\n",
    "            fd.write(str(RHO))\n",
    "        elif PRIVATIZER == \"noise_privatizer\":\n",
    "            print(PRIVATIZER,\"SIGMA=\",SIGMA)\n",
    "            fd.write(str(SIGMA))\n",
    "        elif PRIVATIZER == \"dp_privatizer\":\n",
    "            print(PRIVATIZER,\"EPSILON=\",EPSILON)\n",
    "            fd.write(str(EPSILON))\n",
    "        elif PRIVATIZER == \"MI_privatizer\":\n",
    "            print(PRIVATIZER,\"CODEBOOK MULTIPLIER=\", CODEBOOK_MULTIPLIER)\n",
    "            fd.write(str(CODEBOOK_MULTIPLIER))\n",
    "        fd.write(\",\")\n",
    "\n",
    "        print(\"Privacy Metrics:\", acc, loc_error)\n",
    "        fd.write(str(acc))\n",
    "        fd.write(\",\")\n",
    "        fd.write(str(loc_error))\n",
    "        fd.write(\",\")\n",
    "        print(\"Utility Metrics:\", map_error, distortion, dist_error, density_error)\n",
    "        fd.write(str(map_error))\n",
    "        fd.write(\",\")\n",
    "        fd.write(str(distortion))\n",
    "        fd.write(\",\")\n",
    "        fd.write(str(dist_error))\n",
    "        fd.write(\",\")\n",
    "        fd.write(str(density_error))\n",
    "        fd.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
